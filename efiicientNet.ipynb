{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62b2651a",
   "metadata": {},
   "source": [
    "# Assignment Module 2: Pet Classification\n",
    "\n",
    "The goal of this assignment is to implement a neural network that classifies images of 37 breeds of cats and dogs from the [Oxford-IIIT-Pet dataset](https://www.robots.ox.ac.uk/~vgg/data/pets/). The assignment is divided into two parts: first, you will be asked to implement from scratch your own neural network for image classification; then, you will fine-tune a pretrained network provided by PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff58d6d",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "The following cells contain the code to download and access the dataset you will be using in this assignment. Note that, although this dataset features each and every image from [Oxford-IIIT-Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/), it uses a different train-val-test split than the original authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6ae6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is already cloned to /scratch.hpc/matteo.preda/ipcv-assignment-2\n",
    "# No need to clone here\n",
    "print(\"Dataset location: /scratch.hpc/matteo.preda/ipcv-assignment-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca0d4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/CVLAB-Unibo/ipcv-assignment-2.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb92620",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torch import Tensor\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac12d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Login into wandb\n",
    "API = '287da622db71b3c4376edca2ab62b34407fb7070'\n",
    "wandb.login(API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49d6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS_TO_NAME = {\n",
    "    0: \"Abyssinian\",\n",
    "    1: \"american_bulldog\",\n",
    "    2: \"american_pit_bull_terrier\",\n",
    "    3: \"basset_hound\",\n",
    "    4: \"beagle\",\n",
    "    5: \"Bengal\",\n",
    "    6: \"Birman\",\n",
    "    7: \"Bombay\",\n",
    "    8: \"boxer\",\n",
    "    9: \"British_Shorthair\",\n",
    "    10: \"chihuahua\",\n",
    "    11: \"Egyptian_Mau\",\n",
    "    12: \"english_cocker_spaniel\",\n",
    "    13: \"english_setter\",\n",
    "    14: \"german_shorthaired\",\n",
    "    15: \"great_pyrenees\",\n",
    "    16: \"havanese\",\n",
    "    17: \"japanese_chin\",\n",
    "    18: \"keeshond\",\n",
    "    19: \"leonberger\",\n",
    "    20: \"Maine_Coon\",\n",
    "    21: \"miniature_pinscher\",\n",
    "    22: \"newfoundland\",\n",
    "    23: \"Persian\",\n",
    "    24: \"pomeranian\",\n",
    "    25: \"pug\",\n",
    "    26: \"Ragdoll\",\n",
    "    27: \"Russian_Blue\",\n",
    "    28: \"saint_bernard\",\n",
    "    29: \"samoyed\",\n",
    "    30: \"scottish_terrier\",\n",
    "    31: \"shiba_inu\",\n",
    "    32: \"Siamese\",\n",
    "    33: \"Sphynx\",\n",
    "    34: \"staffordshire_bull_terrier\",\n",
    "    35: \"wheaten_terrier\",\n",
    "    36: \"yorkshire_terrier\",\n",
    "}\n",
    "\n",
    "NUM_CLASSES = 37\n",
    "\n",
    "\n",
    "class OxfordPetDataset(Dataset):\n",
    "    def __init__(self, split: str, transform=None) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.root = Path(\"/scratch.hpc/matteo.preda/ipcv-assignment-2\") / \"dataset\"\n",
    "        # self.root = Path(\"ipcv-assignment-2\") / \"dataset\"\n",
    "        self.split = split\n",
    "        self.names, self.labels = self._get_names_and_labels()\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[Tensor, int]:\n",
    "        img_path = self.root / \"images\" / f\"{self.names[idx]}.jpg\"\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def get_num_classes(self) -> int:\n",
    "        return max(self.labels) + 1\n",
    "\n",
    "    def _get_names_and_labels(self) -> Tuple[List[str], List[int]]:\n",
    "        names = []\n",
    "        labels = []\n",
    "\n",
    "        with open(self.root / \"annotations\" / f\"{self.split}.txt\") as f:\n",
    "            for line in f:\n",
    "                name, label = line.replace(\"\\n\", \"\").split(\" \")\n",
    "                names.append(name),\n",
    "                labels.append(int(label) - 1)\n",
    "\n",
    "        return names, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa3194e",
   "metadata": {},
   "source": [
    "## Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1413d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimized transforms for GPU training\n",
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.RandomCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# No augmentation for val/test, but still resize and normalize\n",
    "val_test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = OxfordPetDataset(\"train\", transform=train_transform)\n",
    "val_dataset = OxfordPetDataset(\"val\", transform=val_test_transform)\n",
    "test_dataset = OxfordPetDataset(\"test\", transform=val_test_transform)\n",
    "\n",
    "# Optimized DataLoaders for GPU training\n",
    "# num_workers: parallelize data loading; pin_memory: faster GPU transfer\n",
    "train_dl = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    "    persistent_workers=True\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    test_dataset, \n",
    "    batch_size=BATCH_SIZE, \n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc6eb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denorm_image(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n",
    "    \"\"\"\n",
    "    Denormalize a tensor image (C, H, W) or (B, C, H, W) that was normalized\n",
    "    using torchvision's Normalize(mean, std).\n",
    "    \"\"\"\n",
    "    if tensor.ndim == 3:\n",
    "        mean = torch.tensor(mean, device=tensor.device).view(-1, 1, 1)\n",
    "        std = torch.tensor(std, device=tensor.device).view(-1, 1, 1)\n",
    "    elif tensor.ndim == 4:\n",
    "        mean = torch.tensor(mean, device=tensor.device).view(1, -1, 1, 1)\n",
    "        std = torch.tensor(std, device=tensor.device).view(1, -1, 1, 1)\n",
    "    else:\n",
    "        raise ValueError(\"Expected tensor of shape (C,H,W) or (B,C,H,W)\")\n",
    "\n",
    "    return tensor * std + mean\n",
    "\n",
    "\n",
    "print(f\"The train dataset contains {len(train_dataset)}\")\n",
    "print(f\"The test dataset contains {len(val_dataset)}\")\n",
    "print(f\"The train dataset contains {len(test_dataset)}\")\n",
    "indexes = torch.randperm(len(train_dataset))[:9]\n",
    "plt.figure(figsize=(15, 15))\n",
    "for ind, i in enumerate(indexes):\n",
    "    img, label = train_dataset[i]\n",
    "    plt.subplot(3, 3, ind + 1)\n",
    "    plt.imshow(denorm_image(img).permute(1, 2, 0))\n",
    "    plt.title(f\"Label= {LABELS_TO_NAME[label]}\")\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670b56c6",
   "metadata": {},
   "source": [
    "## Part 1: design your own network\n",
    "\n",
    "Your goal is to implement a convolutional neural network for image classification and train it from scratch on `OxfordPetDataset`. You should consider yourselves satisfied once you obtain a classification accuracy on the test split of ~60%. You are free to achieve this however you want, except for a few rules you must follow:\n",
    "\n",
    "- Compile this notebook by displaying the results obtained by the best model you found throughout your experimentation; then show how, by removing some of its components, its performance drops. In other words, do an _ablation study_ to prove that your design choices have a positive impact on the final result.\n",
    "\n",
    "- Do not instantiate an off-the-self PyTorch network. Instead, construct your network as a composition of existing PyTorch layers. In more concrete terms, you can use e.g. `torch.nn.Linear`, but you cannot use e.g. `torchvision.models.alexnet`.\n",
    "\n",
    "- Show your results and ablations with plots, tables, images, etc. â€” the clearer, the better.\n",
    "\n",
    "Don't be too concerned with your model performance: the ~60% is just to give you an idea of when to stop. Keep in mind that a thoroughly justified model with lower accuracy will be rewarded more points than a poorly experimentally validated model with higher accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d1f64d",
   "metadata": {},
   "source": [
    "## Block definition for the net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a148bc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Swish(nn.Module):\n",
    "    \"\"\"Swish activation function (SiLU)\"\"\"\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "    \"\"\"Squeeze-and-Excitation block for channel attention\"\"\"\n",
    "    def __init__(self, in_channels: int, reduced_dim: int):\n",
    "        super().__init__()\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, reduced_dim, 1),\n",
    "            Swish(),\n",
    "            nn.Conv2d(reduced_dim, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.se(x)\n",
    "\n",
    "\n",
    "class DropConnect(nn.Module):\n",
    "    \"\"\"Stochastic depth (drop connect) for regularization\"\"\"\n",
    "    def __init__(self, drop_prob: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.drop_prob == 0:\n",
    "            return x\n",
    "        \n",
    "        keep_prob = 1 - self.drop_prob\n",
    "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
    "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
    "        random_tensor.floor_()\n",
    "        return x.div(keep_prob) * random_tensor\n",
    "\n",
    "\n",
    "class MBConvBlock(nn.Module):\n",
    "    \"\"\"Mobile Inverted Bottleneck Convolution Block with SE\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        kernel_size: int,\n",
    "        stride: int,\n",
    "        expand_ratio: int,\n",
    "        se_ratio: float = 0.25,\n",
    "        drop_connect_rate: float = 0.0\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.stride = stride\n",
    "        self.use_residual = (stride == 1 and in_channels == out_channels)\n",
    "        \n",
    "        # Expansion phase\n",
    "        expanded_channels = in_channels * expand_ratio\n",
    "        self.expand = expand_ratio != 1\n",
    "        \n",
    "        if self.expand:\n",
    "            self.expansion_conv = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, expanded_channels, 1, bias=False),\n",
    "                nn.BatchNorm2d(expanded_channels),\n",
    "                Swish()\n",
    "            )\n",
    "        \n",
    "        # Depthwise separable convolution\n",
    "        self.depthwise_conv = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                expanded_channels,\n",
    "                expanded_channels,\n",
    "                kernel_size,\n",
    "                stride=stride,\n",
    "                padding=kernel_size // 2,\n",
    "                groups=expanded_channels,  # Depthwise\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(expanded_channels),\n",
    "            Swish()\n",
    "        )\n",
    "        \n",
    "        # Squeeze and Excitation\n",
    "        se_channels = max(1, int(in_channels * se_ratio))\n",
    "        self.se = SqueezeExcitation(expanded_channels, se_channels)\n",
    "        \n",
    "        # Output projection\n",
    "        self.project = nn.Sequential(\n",
    "            nn.Conv2d(expanded_channels, out_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels)\n",
    "        )\n",
    "        \n",
    "        self.drop_connect = DropConnect(drop_connect_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        \n",
    "        if self.expand:\n",
    "            x = self.expansion_conv(x)\n",
    "        \n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.se(x)\n",
    "        x = self.project(x)\n",
    "        \n",
    "        if self.use_residual:\n",
    "            x = self.drop_connect(x)\n",
    "            x = x + identity\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96adcdf",
   "metadata": {},
   "source": [
    "## Net definition and variation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ae0306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetCustom(nn.Module):\n",
    "    \"\"\"Custom EfficientNet-inspired model built from scratch\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 37,\n",
    "        width_coefficient: float = 1.0,\n",
    "        depth_coefficient: float = 1.0,\n",
    "        dropout_rate: float = 0.2,\n",
    "        drop_connect_rate: float = 0.2,\n",
    "        image_size: int = 224\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        # EfficientNet block configuration\n",
    "        # Format: [expand_ratio, out_channels, num_blocks, stride, kernel_size]\n",
    "        base_config = [\n",
    "            [1, 16, 1, 1, 3],   # Stage 1\n",
    "            [6, 24, 2, 2, 3],   # Stage 2\n",
    "            [6, 40, 2, 2, 5],   # Stage 3\n",
    "            [6, 80, 3, 2, 3],   # Stage 4\n",
    "            [6, 112, 3, 1, 5],  # Stage 5\n",
    "            [6, 192, 4, 2, 5],  # Stage 6\n",
    "            [6, 320, 1, 1, 3]   # Stage 7\n",
    "        ]\n",
    "        \n",
    "        # Scaling functions\n",
    "        def scale_width(channels):\n",
    "            return int(math.ceil(channels * width_coefficient / 8) * 8)\n",
    "        \n",
    "        def scale_depth(num_blocks):\n",
    "            return int(math.ceil(num_blocks * depth_coefficient))\n",
    "        \n",
    "        # Stem: initial convolution\n",
    "        out_channels = scale_width(32)\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, out_channels, 3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            Swish()\n",
    "        )\n",
    "        \n",
    "        # Build MBConv stages\n",
    "        blocks = []\n",
    "        total_blocks = sum([scale_depth(config[2]) for config in base_config])\n",
    "        block_idx = 0\n",
    "        \n",
    "        in_channels = out_channels\n",
    "        for expand_ratio, channels, num_blocks, stride, kernel_size in base_config:\n",
    "            out_channels = scale_width(channels)\n",
    "            num_blocks = scale_depth(num_blocks)\n",
    "            \n",
    "            for i in range(num_blocks):\n",
    "                # Linearly increase drop connect rate\n",
    "                drop_rate = drop_connect_rate * block_idx / total_blocks\n",
    "                \n",
    "                blocks.append(\n",
    "                    MBConvBlock(\n",
    "                        in_channels=in_channels,\n",
    "                        out_channels=out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride if i == 0 else 1,\n",
    "                        expand_ratio=expand_ratio,\n",
    "                        se_ratio=0.25,\n",
    "                        drop_connect_rate=drop_rate\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "                block_idx += 1\n",
    "        \n",
    "        self.blocks = nn.Sequential(*blocks)\n",
    "        \n",
    "        # Head: final classification layers\n",
    "        head_channels = scale_width(1280)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, head_channels, 1, bias=False),\n",
    "            nn.BatchNorm2d(head_channels),\n",
    "            Swish(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(head_channels, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize network weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5bbb789",
   "metadata": {},
   "source": [
    "## Model factory functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b87a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_efficientnet_b0(num_classes: int = 37):\n",
    "    \"\"\"EfficientNet-B0: baseline model\"\"\"\n",
    "    return EfficientNetCustom(\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=1.0,\n",
    "        depth_coefficient=1.0,\n",
    "        dropout_rate=0.2,\n",
    "        drop_connect_rate=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "def create_efficientnet_b1(num_classes: int = 37):\n",
    "    \"\"\"EfficientNet-B1: slightly larger\"\"\"\n",
    "    return EfficientNetCustom(\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=1.0,\n",
    "        depth_coefficient=1.1,\n",
    "        dropout_rate=0.2,\n",
    "        drop_connect_rate=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "def create_efficientnet_b2(num_classes: int = 37):\n",
    "    \"\"\"EfficientNet-B2: medium size\"\"\"\n",
    "    return EfficientNetCustom(\n",
    "        num_classes=num_classes,\n",
    "        width_coefficient=1.1,\n",
    "        depth_coefficient=1.2,\n",
    "        dropout_rate=0.4,\n",
    "        drop_connect_rate=0.2\n",
    "    )\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d18e865",
   "metadata": {},
   "source": [
    "## Model tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb5e333",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing EfficientNet Custom Implementation\\n\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test model creation\n",
    "model_b0 = create_efficientnet_b0(NUM_CLASSES)\n",
    "model_b1 = create_efficientnet_b1(NUM_CLASSES)\n",
    "model_b2 = create_efficientnet_b2(NUM_CLASSES)\n",
    "\n",
    "# Print parameter counts\n",
    "models_info = {\n",
    "    'EfficientNet-B0': model_b0,\n",
    "    'EfficientNet-B1': model_b1,\n",
    "    'EfficientNet-B2': model_b2\n",
    "}\n",
    "\n",
    "for name, model in models_info.items():\n",
    "    params = count_parameters(model)\n",
    "    print(f\"{name:30s}: {params:,} parameters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randn(2, 3, 224, 224)\n",
    "test_output = model_b0(test_input)\n",
    "print(f\"\\nForward pass test:\")\n",
    "print(f\"  Input shape:  {test_input.shape}\")\n",
    "print(f\"  Output shape: {test_output.shape}\")\n",
    "print(f\"  Output range: [{test_output.min().item():.3f}, {test_output.max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0f00a3",
   "metadata": {},
   "source": [
    "## Train and Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a84d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_efficientnet(\n",
    "    model: torch.nn.Module,\n",
    "    dl_train,\n",
    "    dl_val,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    epochs: int,\n",
    "    name: str,\n",
    "    device: str = \"cpu\",\n",
    "    project: str = \"IPCV2-EfficientNet\",\n",
    "    run_name: str = None,\n",
    "    save_dir: str = \"./checkpoints\",\n",
    "    use_amp: bool = True,\n",
    "    grad_clip: float = 1.0\n",
    "):\n",
    "    \"\"\"Enhanced training with mixed precision and gradient clipping\"\"\"\n",
    "    \n",
    "    wandb.init(\n",
    "        project=project,\n",
    "        name=run_name,\n",
    "        entity=\"mpreda01-universit-di-bologna\",\n",
    "        config={\n",
    "            \"epochs\": epochs,\n",
    "            \"optimizer\": optimizer.__class__.__name__,\n",
    "            \"scheduler\": scheduler.__class__.__name__ if scheduler else \"None\",\n",
    "            \"criterion\": criterion.__class__.__name__,\n",
    "            \"device\": device,\n",
    "            \"model\": model.__class__.__name__,\n",
    "            \"use_amp\": use_amp,\n",
    "            \"grad_clip\": grad_clip\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    os.makedirs(os.path.join(save_dir, name), exist_ok=True)\n",
    "    \n",
    "    model.to(device)\n",
    "    scaler = torch.cuda.amp.GradScaler() if use_amp and device == \"cuda\" else None\n",
    "    \n",
    "    best_val_acc = 0.0\n",
    "    global_step = 0\n",
    "    \n",
    "    for e in tqdm(range(epochs), desc=\"Training\"):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, labels in dl_train:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed precision forward pass\n",
    "            if use_amp and scaler:\n",
    "                with torch.cuda.amp.autocast(\"cuda\"):\n",
    "                    output_logits = model(images)\n",
    "                    loss = criterion(output_logits, labels)\n",
    "                \n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient clipping\n",
    "                if grad_clip > 0:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                output_logits = model(images)\n",
    "                loss = criterion(output_logits, labels)\n",
    "                loss.backward()\n",
    "                \n",
    "                if grad_clip > 0:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "                \n",
    "                optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            global_step += 1\n",
    "            \n",
    "            if global_step % 10 == 0:\n",
    "                wandb.log({\"train_loss_step\": loss.item()}, step=global_step)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in dl_val:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                \n",
    "                if use_amp and device == \"cuda\":\n",
    "                    with torch.cuda.amp.autocast():\n",
    "                        output_logits = model(images)\n",
    "                        loss = criterion(output_logits, labels)\n",
    "                else:\n",
    "                    output_logits = model(images)\n",
    "                    loss = criterion(output_logits, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                pred_labels = output_logits.argmax(1)\n",
    "                correct += (pred_labels == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        avg_val_loss = val_loss / len(dl_val)\n",
    "        val_accuracy = correct / total\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "            current_lr = scheduler.get_last_lr()[0]\n",
    "            wandb.log({\"learning_rate\": current_lr}, step=global_step)\n",
    "        \n",
    "        # Logging\n",
    "        wandb.log({\n",
    "            \"val_loss_epoch\": avg_val_loss,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"epoch\": e + 1\n",
    "        }, step=global_step)\n",
    "        \n",
    "        # # Save checkpoint\n",
    "        # checkpoint_path = os.path.join(save_dir, name, f\"epoch_{e+1}.pt\")\n",
    "        # torch.save({\n",
    "        #     \"epoch\": e + 1,\n",
    "        #     \"model_state_dict\": model.state_dict(),\n",
    "        #     \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        #     \"scheduler_state_dict\": scheduler.state_dict() if scheduler else None,\n",
    "        #     \"val_loss\": avg_val_loss,\n",
    "        #     \"val_accuracy\": val_accuracy,\n",
    "        # }, checkpoint_path)\n",
    "        \n",
    "        # # Save best model\n",
    "        # if val_accuracy > best_val_acc:\n",
    "        #     best_val_acc = val_accuracy\n",
    "        #     best_path = os.path.join(save_dir, name, \"best_model.pt\")\n",
    "        #     torch.save(model.state_dict(), best_path)\n",
    "        #     wandb.run.summary[\"best_val_accuracy\"] = best_val_acc\n",
    "        #     print(f\"\\nSaved best model (epoch {e+1}, acc={best_val_acc:.4f})\")\n",
    "    \n",
    "    wandb.finish()\n",
    "\n",
    "def test(\n",
    "    model: torch.nn.Module,\n",
    "    dl_test,\n",
    "    criterion,\n",
    "    checkpoint_path: str,\n",
    "    device: str = \"cpu\",\n",
    "    project: str = \"my_project\",\n",
    "    run_name: str = \"test_run\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate a trained model on the test set and compute performance metrics.\n",
    "\n",
    "    Args:\n",
    "        model: Trained torch.nn.Module\n",
    "        dl_test: DataLoader for the test set\n",
    "        criterion: Loss function\n",
    "        checkpoint_path: Path to a saved model checkpoint (.pt)\n",
    "        device: Device to run inference on ('cpu' or 'cuda')\n",
    "        project: W&B project name\n",
    "        run_name: W&B run name for logging results\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize W&B\n",
    "    wandb.init(\n",
    "        project=project,\n",
    "        name=run_name,\n",
    "        entity=\"mpreda01-universit-di-bologna\",\n",
    "        job_type=\"test\",\n",
    "        config={\n",
    "            \"device\": device,\n",
    "            \"model\": model.__class__.__name__,\n",
    "            \"checkpoint\": checkpoint_path,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Load model checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    if \"model_state_dict\" in checkpoint:\n",
    "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)  # if saved with model.state_dict() only\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Accumulators\n",
    "    test_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in dl_test:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            # (B, 3, 224, 224)\n",
    "            # (B, N)\n",
    "            # (B,)\n",
    "            output_logits = model(images)\n",
    "            loss = criterion(output_logits, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            preds = output_logits.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Average loss\n",
    "    avg_test_loss = test_loss / len(dl_test)\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    prec = precision_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    rec = recall_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\", zero_division=0)\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    # Print report\n",
    "    print(\"\\n=== Test Results ===\")\n",
    "    print(f\"Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1-score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "\n",
    "    # Log metrics to W&B\n",
    "    wandb.log(\n",
    "        {\n",
    "            \"test_loss\": avg_test_loss,\n",
    "            \"test_accuracy\": acc,\n",
    "            \"test_precision\": prec,\n",
    "            \"test_recall\": rec,\n",
    "            \"test_f1\": f1,\n",
    "            \"confusion_matrix\": wandb.plot.confusion_matrix(\n",
    "                probs=None,\n",
    "                y_true=all_labels,\n",
    "                preds=all_preds,\n",
    "                title=\"Confusion Matrix\",\n",
    "            ),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "    return {\n",
    "        \"test_loss\": avg_test_loss,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"confusion_matrix\": cm,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bc204c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 150\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-3\n",
    "CRITERION = nn.CrossEntropyLoss()\n",
    "\n",
    "for model_name, model in models_info.items():\n",
    "    print(f\"Training {model_name}...\")  \n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "    train_efficientnet(\n",
    "        model=model,\n",
    "        dl_train=train_dl,\n",
    "        dl_val=val_dl,\n",
    "        criterion=CRITERION,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        epochs=EPOCHS,\n",
    "        name=model_name,\n",
    "        project=\"IPCV2-EfficientNet\",\n",
    "        run_name=f\"{model_name}_run\",\n",
    "        use_amp=True,\n",
    "        grad_clip=1.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b6c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name, model in models_info.items():\n",
    "    print(f\"\\nTesting {model_name}...\")\n",
    "    test_metrics = test(\n",
    "            model,\n",
    "            test_dl,\n",
    "            CRITERION,\n",
    "            checkpoint_path=f\"./checkpoints/{model_name}/best_model.pt\",\n",
    "            project=\"IPCV2-EfficientNet\",\n",
    "            run_name=f\"{model_name}_test\"\n",
    "        )\n",
    "        \n",
    "    print(f\"\\nTest Results for {model_name}:\")\n",
    "    print(f\"  Accuracy:  {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {test_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score:  {test_metrics['f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204bcc45",
   "metadata": {},
   "source": [
    "## Definition of all the experiments to perform the ablation study"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipcv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
